{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c4ee62-8c21-426b-b088-b7b8ee9c7427",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import arrays_zip, explode, col, to_timestamp, to_date, date_format, current_timestamp\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    LongType, StringType, TimestampType\n",
    ")\n",
    "from pyspark.errors import PySparkException \n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure Spark session is available\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "air_qty_df = spark.table(\"air_quality_project.air_quality_bronze_data.air_quality_data\")\n",
    "\n",
    "process_name = \"air_quality_load_job\"\n",
    "\n",
    "error_schema = StructType([\n",
    "    #StructField(\"ID\", LongType(), False),\n",
    "    StructField(\"Process_name\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"error_time\", TimestampType(), True),\n",
    "    StructField(\"error_code\", StringType(), True)\n",
    "])\n",
    "\n",
    "try:\n",
    "    df_zip_array_col = air_qty_df.withColumn(\n",
    "        \"hourly\",\n",
    "        explode(\n",
    "            arrays_zip(\n",
    "                col(\"`hourly.time`\"),\n",
    "                col(\"`hourly.pm10`\"),\n",
    "                col(\"`hourly.pm2_5`\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_struct_col = df_zip_array_col.select(\n",
    "        col(\"latitude\"),\n",
    "        col(\"longitude\"),\n",
    "        col(\"elevation\"),\n",
    "        col(\"timezone\"),\n",
    "        to_timestamp(col(\"hourly.`hourly.time`\")).alias(\"timestamp\"),\n",
    "        col(\"hourly.`hourly.pm10`\").alias(\"pm10\"),\n",
    "        col(\"hourly.`hourly.pm2_5`\").alias(\"pm2_5\")\n",
    "    )\n",
    "\n",
    "    df_silver = (\n",
    "        df_struct_col\n",
    "        .withColumn(\"date\", to_date(\"timestamp\"))\n",
    "        .withColumn(\"time\", date_format(\"timestamp\", \"HH:mm:ss\"))   \n",
    "        .withColumn(\"insertdate\", current_timestamp()) \n",
    "        .dropna()\n",
    "        .dropDuplicates()\n",
    "    )\n",
    "\n",
    "    df_silver.createOrReplaceTempView(\"people_view\")\n",
    "\n",
    "    #sql_query = \"SELECT 1/0 FROM people_view\"\n",
    "    result_df = spark.sql(sql_query)\n",
    "\n",
    "    # Display the result\n",
    "    result_df.show()\n",
    "\n",
    "    #display(df_silver)\n",
    "\n",
    "    df_silver.write.format(\"delta\").mode(\"append\").saveAsTable(\"air_quality_project.air_quality_silver_data.hourly_air_quality\")\n",
    "\n",
    "except PySparkException as e:\n",
    "    # Handle the specific PySpark error\n",
    "  \n",
    "\n",
    "    \n",
    "    error_row = Row(\n",
    "        #ID=int(time.time()),  # unique ID using epoch\n",
    "        Process_name=process_name,\n",
    "        error_message=str(e),\n",
    "        error_time=datetime.now(),\n",
    "        error_code=(e.getCondition() if hasattr(e, \"getCondition\") else type(e).__name__)\n",
    "    )\n",
    "\n",
    "    # Create DataFrame from Row with schema\n",
    "    error_df = spark.createDataFrame([error_row], schema=error_schema)\n",
    "\n",
    "\n",
    "    # Write error log to Delta\n",
    "    error_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"log_master.process_log.error_log\")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle other general Python exceptions\n",
    "    print(f\"A general Python error occurred: {e}\")\n",
    "finally:\n",
    "    print(\"Spark session stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Air_quality_clean_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
